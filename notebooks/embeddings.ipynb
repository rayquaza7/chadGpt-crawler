{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean data, create embeddings and upload to pinecone\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import openai\n",
    "import pinecone\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "EMBEDDINGS_MODEL = \"text-embedding-ada-002\"\n",
    "EMBEDDINGS_DIMENSION = 1536\n",
    "EMBEDDINGS_MAX_TOKENS = 8191\n",
    "COMPLETION_MODEL = \"text-davinci-003\"\n",
    "DAVINCI_MAX_TOKENS = 4096\n",
    "\n",
    "PINECONE_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),\n",
    "    environment=\"us-west1-gcp\"\n",
    ")\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "%load_ext sql\n",
    "%sql $DATABASE_URL\n",
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 tokens ~ 750 words; there is no way to get the number of tokens from the API for 2nd gen models for now\n",
    "# 1 token ~ 4 characters; ceil to be safe\n",
    "def token_estimate(text):\n",
    "    return int(math.ceil(len(text) / 4))\n",
    "\n",
    "# we know that openai ada model costs $0.0004 / 1K tokens\n",
    "def cost_estimate(tokens):\n",
    "    return tokens / 1000 * 0.0004\n",
    "\n",
    "# get embeddings for text\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=EMBEDDINGS_MODEL,\n",
    "        input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def get_pinecone_index(index_name: str) -> pinecone.Index:\n",
    "    return pinecone.Index(index_name)\n",
    "\n",
    "def set_embedded_status(df: pd.DataFrame):\n",
    "    # the rows that have will be added to pinecone index, set their embedded status to 1 in database\n",
    "    df['embedded'] = 1\n",
    "    df.to_sql('crawl_data', engine, if_exists='replace', index=False)\n",
    "\n",
    "def add_to_pinecone(df: pd.DataFrame, index: pinecone.Index):\n",
    "    # get embeddings for each row and add to pinecone index\n",
    "    for i in tqdm(range(0, df.shape[0], PINECONE_BATCH_SIZE)):\n",
    "        # set end position of batch\n",
    "        i_end = min(i+PINECONE_BATCH_SIZE, df.shape[0])\n",
    "        ids_batch = [str(n) for n in range(i, i_end)]\n",
    "        metadata_batch = df.iloc[i: i_end].to_dict('records')\n",
    "        # create embeddings for the data column in df\n",
    "        embeds = [get_embedding(text) for text in df.iloc[i: i_end]['data']]\n",
    "        to_upsert = zip(ids_batch, embeds, metadata_batch)\n",
    "        index.upsert(vectors=list(to_upsert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # convert data column into list of p texts. data is a list of p tags but when u read from db, it is a string\n",
    "    df['data'] = df['data'].apply(lambda x: re.findall(r'\\\"(.*?)\\\"', x))\n",
    "\n",
    "    # remove duplicate urls\n",
    "    old_rows = df.shape[0]\n",
    "    df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "    print('duplicate urls deleted:', old_rows - df.shape[0])\n",
    "\n",
    "    # delete duplicate titles\n",
    "    old_rows = df.shape[0]\n",
    "    df = df.drop_duplicates(subset=['title'], keep='first')\n",
    "    print('duplicate titles deleted:', old_rows - df.shape[0])\n",
    "\n",
    "    # remove most common words as they are not informative\n",
    "    # flatten list of lists\n",
    "    data = list(chain.from_iterable(df['data']))\n",
    "    # get most common words, returns list of tuples\n",
    "    most_common = Counter(data).most_common(10)\n",
    "    # convert to list of words\n",
    "    most_common = [x[0] for x in most_common]\n",
    "    # remove 10 most common words from lists of data\n",
    "    df['data'] = df['data'].apply(lambda x: [word for word in x if word not in most_common])\n",
    "\n",
    "    # remove empty data\n",
    "    old_rows = df.shape[0]\n",
    "    df = df[df['data'].apply(lambda x: len(x) > 0)]\n",
    "    print('empty data deleted:', old_rows - df.shape[0])\n",
    "\n",
    "    # concat data into string\n",
    "    df['data'] = df['data'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    # add token estimate column\n",
    "    df['token_estimate'] = df['data'].apply(lambda x: token_estimate(x))\n",
    "    # davinci token limit prompt + completion is 4096\n",
    "    # so we will keep our request token to 2500 (excluding prompt suffix since its tiny)\n",
    "    # if there is anything above 2500 tokens we will get rid of it for now\n",
    "    # we will also get rid of anything below 20 tokens\n",
    "\n",
    "    # remove rows with token estimate above 2500\n",
    "    old_rows = df.shape[0]\n",
    "    df = df[df['token_estimate'] < 2500]\n",
    "    print('rows deleted token estimate above 2500', old_rows - df.shape[0])\n",
    "\n",
    "    # remove rows with token estimate below 20\n",
    "    old_rows = df.shape[0]\n",
    "    df = df[df['token_estimate'] > 20]\n",
    "    print('rows deleted token estimate below 20', old_rows - df.shape[0])\n",
    "\n",
    "    # metadata size limit is 10KB for pinecone\n",
    "    # so we will remove rows with metadata size above 10240 bytes\n",
    "    df['metadata_size'] = df.apply(lambda x: sys.getsizeof(x.to_json()), axis=1)\n",
    "    # remove rows with metadata size above 10240\n",
    "    old_rows = df.shape[0]\n",
    "    df = df[df['metadata_size'] < 10240]\n",
    "    # remove metadata size column\n",
    "    df = df.drop(columns=['metadata_size'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I store crawl data in a postgres db, columns: id:int, url:text, title:text, data:text(list of p tags text), company_id:int(what company ur indexed data belongs to)\n",
    "# there is another table called company which has id:int, name:text (also the pinceone index name)\n",
    "\n",
    "COMPANY_ID = 1\n",
    "\n",
    "# get index name from company table\n",
    "index_name = engine.execute('SELECT * FROM company')\n",
    "# get name where id is COMPANY_ID\n",
    "index_name = [x[1] for x in index_name if x[0] == COMPANY_ID][0]\n",
    "index = get_pinecone_index(index_name)\n",
    "\n",
    "# get all rows that are not embedded\n",
    "df = pd.read_sql(f'SELECT * FROM crawl_data WHERE embedded = 0 AND company_id = {COMPANY_ID}', engine)\n",
    "df = cleanup(df)\n",
    "add_to_pinecone(df, index)\n",
    "set_embedded_status(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83806723b6362d6eeb841b04060d9b3ad7d66bf3bcd76d4da9da2ffb06253ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
